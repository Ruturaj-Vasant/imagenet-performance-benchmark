\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,margin=1in]{geometry}
\usepackage{graphicx}
% Search images in both when building from repo root and Report/
\graphicspath{{results/}{../results/}}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{float}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{subcaption}
% Tighter figure captions and spacing
\captionsetup[figure]{font=small}
\setlength{\abovecaptionskip}{4pt}
\setlength{\belowcaptionskip}{4pt}
\setlength{\textfloatsep}{8pt}
\setlength{\floatsep}{8pt}
\usepackage{longtable}
\usepackage{tabularx}

\title{Performance Benchmarking of Neural Networks Across CPU and GPU Environments}
\author{Ruturaj Tambe}
\date{}

\begin{document}
\maketitle

\begin{abstract}
This report evaluates the performance of neural network workloads across 
local CPU, cloud CPU, and GPU environments. Using four neural network 
architectures and a standardized training configuration, we measure 
runtime, throughput, achieved FLOPs, and efficiency. Roofline models are 
constructed to analyze compute vs. memory bottlenecks, and a batch-size 
sweep experiment is conducted to study scaling effects.
\end{abstract}

\tableofcontents
\newpage

% =============================================================
\section{Experiment Design }
% =============================================================

\subsection{Objective}
The objective is to compare the performance of neural network training 
workloads across different compute environments (local CPU, cloud CPU, GPU). 
The experiment measures throughput, latency, memory usage, and achieved FLOPs 
for multiple neural network models.

\subsection{Hypothesis}
\begin{itemize}
    \item GPUs will deliver significantly higher throughput than CPUs, 
    especially for deeper models with high arithmetic intensity.
    \item CPU vs GPU speedup will vary depending on model complexity.
    \item Increasing batch size should improve GPU utilization and move the workload 
    closer to the compute roofline.
\end{itemize}

\subsection{Experimental Scenarios}
\begin{itemize}
    \item \textbf{Scenario A: Environment Comparison}  
    Four models run under identical configurations on:
    \begin{itemize}
        \item Local CPU
        \item Cloud CPU
        \item GPU (local/cloud)
    \end{itemize}
    \item \textbf{Scenario B: Batch Size Sweep}  
    Model 4 is run with multiple batch sizes on local machine to analyze scaling.
\end{itemize}

\subsection{Experimental Controls}
To ensure that the comparisons across environments were scientifically valid, the following controls were enforced:
\begin{itemize}
    \item \textbf{Identical dataset}: Tiny-ImageNet was used across all runs. It is large enough to stress compute and memory, yet small enough for repeated benchmarking.
    \item \textbf{Same training hyperparameters}: 1 epoch, same optimizer and learning rate, and identical transforms on every device to avoid confounders.
    \item \textbf{Consistent batch sizes}: The same batch sizes were used across environments, except where instability occurred (e.g., SqueezeNet at large batch sizes), in which case the smallest stable batch was used consistently.
    \item \textbf{Same code path and PyTorch family}: The same benchmark script and PyTorch version family were used on all machines to avoid backend-level inconsistencies.
    \item \textbf{Max-batch control}: A fixed maximum number of batches ensured full-epoch execution and avoided misleading results caused by truncated batches on accelerators.
    \item \textbf{Input pipeline parity}: DataLoader workers, shuffling, and augmentation were kept identical.
\end{itemize}


% =============================================================
\section{Environment Setup}
% =============================================================

\subsection{Hardware Configurations}

\subsubsection{Local Machine}
\begin{itemize}
    \item CPU: \textit{Apple M3 Pro (arm64)}
    \item GPU: \textit{Apple M3 Pro integrated GPU (Metal/MPS backend)}
    \item Memory: \textit{18~GB unified memory}
    \item OS: \textit{macOS 15 (Darwin 25.0.0)}
    \item CUDA / PyTorch Versions: \textit{CUDA N/A; PyTorch 2.9.1 (MPS backend)}
\end{itemize}

\subsubsection{Cloud CPU (e2-standard-4)}
\begin{itemize}
    \item 4 vCPUs (Intel/AMD)
    \item 16 GB RAM
    \item Debian 12
    \item Python 3.11, PyTorch CPU
\end{itemize}

\subsubsection{Cloud GPU (Tesla T4)}
\begin{itemize}
    \item GPU: Tesla T4, 16 GB GDDR6
    \item CUDA 12.4, Driver 550.xx
    \item PyTorch 2.8.x + cu126
\end{itemize}

\subsection{Models Evaluated}
\begin{itemize}
    \item \texttt{resnet18}
    \item \texttt{mobilenet\_v2}
    \item \texttt{resnet50}
    \item \texttt{squeezenet1\_1}
\end{itemize}

\subsection{Training Configuration}
\begin{itemize}
    \item Dataset: Tiny-ImageNet (200 classes, 64x64 images)
    \item Epochs: 1
    \item Workers: 4
    \item Batch sizes:
    \begin{itemize}
        \item 128 for all models except SqueezeNet
        \item 64 for SqueezeNet to avoid NaNs
    \end{itemize}
\end{itemize}


% =============================================================
\section{Complexity Estimation }
% =============================================================

\subsection{Model Complexity Summary}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Model & Params (M) & FLOPs/Image (G) & Notes \\
\midrule
ResNet18 & 11.69 & 1.824 & -- \\
MobileNetV2 & 3.50 & 0.328 & Depthwise convs \\
ResNet50 & 25.56 & 4.134 & Bottleneck blocks \\
SqueezeNet1.1 & 1.24 & 0.349 & Fire modules \\
\bottomrule
\end{tabular}
\caption{Model Complexity Estimates (per 224$\times$224 input)}
\end{table}

\subsection{Arithmetic Intensity}
Arithmetic intensity (AI) is defined as:
\begin{equation}
    \mathrm{AI} = \frac{\text{FLOPs}}{\text{Bytes moved}}\, .
\end{equation}
Since precise memory-traffic measurements require hardware counters (not available on CPU/MPS here), we approximate memory traffic using model parameter size, which still separates memory-bound from compute-bound models:
\begin{equation}
    \text{Bytes moved} \approx \text{Model size in bytes},\quad
    \Rightarrow\; \mathrm{AI} \approx \frac{\text{FLOPs per image}}{\text{Model size (bytes)}}\, .
\end{equation}

\begin{table}[H]
\centering
\begin{tabularx}{\textwidth}{lcccX}
\toprule
Model & FLOPs/Image (G) & Model Size (MB) & AI (FLOPs/Byte) & Interpretation \\
\midrule
ResNet18 & 1.824  & 44.67 & 0.0408 & Balanced; compute grows with depth \\
MobileNetV2 & 0.3275 & 13.63 & 0.0240 & Depthwise convs $\Rightarrow$ memory-bound \\
ResNet50 & 4.1337 & 97.81 & 0.0423 & Compute-heavy $\Rightarrow$ high GPU benefit \\
SqueezeNet1.1 & 0.349  & 8.90  & 0.0392 & Light model, moderate AI \\
\bottomrule
\end{tabularx}
\caption{Estimated arithmetic intensity using model size as a proxy for bytes moved.}
\end{table}

MobileNetV2 has the lowest AI, making it the most memory-bound (confirmed later in the roofline plots). ResNet50 has the highest AI and benefits most from GPU compute. SqueezeNet and ResNet18 sit mid-range and are moderately balanced.


% =============================================================
\section{Measurement }
% =============================================================

\subsection{Metrics Collected}
\begin{itemize}
    \item Time per batch / epoch
    \item Throughput (images/sec)
    \item Achieved GFLOPs/sec or TFLOPs/sec
    \item GPU/CPU utilization
    \item Memory footprint
\end{itemize}

\subsection{Data Collection Method}
All runs were executed using the same Python benchmark script with CSV logging enabled.

\subsection{Measurement Fairness}
To ensure fairness in measurement, all environments executed the exact same Python benchmark script, used the same Tiny-ImageNet dataset, and logged results to standardized CSV templates. The runs were performed under low system load to avoid OS scheduling interference. SqueezeNet1.1 was benchmarked with the smallest stable batch size across all devices when instability was observed with larger batches. These steps help ensure that performance differences arise from hardware characteristics rather than experimental inconsistencies.

% =============================================================
\section{Results}
% =============================================================

\subsection{CPU vs GPU Results (Scenario A)}

\subsubsection*{Table 1: Performance Summary (Local M3 vs Cloud CPU vs Cloud GPU)}
\begin{table}[H]
\centering
\begin{tabular}{llccccc}
\toprule
Model & Device & Train Thr. & Val Thr. & TFLOPs (train) & TFLOPs (val) & \shortstack{Peak GPU\\Mem (MB)} \\
 &  & (img/s) & (img/s) &  &  &  \\
\midrule
\multicolumn{7}{l}{\textbf{ResNet18}} \\
 & MPS (Apple GPU) & 110.07 & 189.56 & 0.306 & 0.850 & N/A \\
 & CPU & 8.88 & 27.64 & 0.016 & 0.050 & N/A \\
 & GPU (T4) & 315.74 & 412.36 & 0.592 & 0.775 & 3135 \\
\midrule
\multicolumn{7}{l}{\textbf{MobileNetV2}} \\
 & MPS (Apple GPU) & 99.64 & 177.36 & 0.047 & 0.131 & N/A \\
 & CPU & 11.71 & 47.94 & 0.004 & 0.016 & N/A \\
 & GPU (T4) & 250.47 & 379.06 & 0.082 & 0.128 & 10278 \\
\midrule
\multicolumn{7}{l}{\textbf{ResNet50}} \\
 & MPS (Apple GPU) & 50.60 & 92.13 & 0.249 & 0.535 & N/A \\
 & CPU & 3.37 & 11.78 & 0.014 & 0.049 & N/A \\
 & GPU (T4) & 98.45 & 312.44 & 0.407 & 1.297 & 11345 \\
\midrule
\multicolumn{7}{l}{\textbf{SqueezeNet1.1}} \\
 & MPS (Apple GPU) & 178.03 & 228.73 & 0.140 & 0.281 & N/A \\
 & CPU & 24.32 & 60.35 & 0.0085 & 0.0211 & N/A \\
 & GPU (T4) & 346.57 & 407.77 & 0.124 & 0.148 & 1503 \\
\bottomrule
\end{tabular}
\caption{Throughput, achieved compute, and memory footprint across environments.}
\end{table}

\subsubsection*{Table 2: Epoch Time Comparison}
\begin{table}[H]
\centering
\begin{tabular}{l l cc}
\toprule
Model & Device & Train Epoch Time (s) & Val Epoch Time (s) \\
\midrule
ResNet18 & MPS & 58.15 & 33.76 \\
 & CPU & 720.89 & 231.59 \\
 & GPU (T4) & 20.27 & 15.52 \\
\midrule
MobileNetV2 & MPS & 64.23 & 36.09 \\
 & CPU & 546.57 & 133.51 \\
 & GPU (T4) & 25.55 & 16.88 \\
\midrule
ResNet50 & MPS & 126.49 & 69.47 \\
 & CPU & 1899.62 & 543.22 \\
 & GPU (T4) & 65.01 & 20.48 \\
\midrule
SqueezeNet1.1 & MPS & 35.95 & 27.98 \\
 & CPU & 263.17 & 106.05 \\
 & GPU (T4) & 9.23 & 7.85 \\
\bottomrule
\end{tabular}
\caption{End-to-end epoch times per environment.}
\end{table}

\subsubsection*{Table 3: System Efficiency (Utilization)}
\begin{table}[H]
\centering
\begin{tabular}{l l cc}
\toprule
Model & Device & GPU Util (\%) & CPU Util (\%) \\
\midrule
ResNet18 & MPS & N/A & 14.8 \\
 & CPU & N/A & 2.4 \\
 & GPU (T4) & 0 & 5 \\
\midrule
MobileNetV2 & MPS & N/A & 21.7 \\
 & CPU & N/A & 0 \\
 & GPU (T4) & 1 & 0 \\
\midrule
ResNet50 & MPS & N/A & 17.9 \\
 & CPU & N/A & 0 \\
 & GPU (T4) & 3 & 0 \\
\midrule
SqueezeNet1.1 & MPS & N/A & 13.2 \\
 & CPU & N/A & 0 \\
 & GPU (T4) & 0 & 10.5 \\
\bottomrule
\end{tabular}
\caption{Observed processor and accelerator utilization.}
\end{table}

\subsection{Batch Size Sweep (Scenario B)}
\label{sec:batch-sweep}
\textbf{Model: SqueezeNet1.1 on Local Machine (Apple M3 Pro, MPS).} Using three CSVs (batch sizes 128, 256, 512), we summarize the effect of batch size:

\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
Batch Size & Throughput (img/s) & Achieved GFLOPs/s \\
\midrule
32  & $\sim$160 & $\sim$50  \\
64  & $\sim$300 & $\sim$80  \\
128 & \textit{NaNs / unstable} & \textit{invalid} \\
\bottomrule
\end{tabular}
\caption{Batch size scaling for SqueezeNet1.1 (GPU). Batch 64 offered the best trade-off; batch 128 was unstable on T4 under PyTorch~2.8.}
\end{table}

\paragraph{Interpretation}
\begin{itemize}
  \item SqueezeNet is sensitive to batch size on GPU; throughput and GFLOPs/s improve from 32 to 64.
  \item Batch 64 is optimal in these runs, with balanced utilization and memory headroom.
  \item Batch 128 produced numerical instability (NaNs), likely due to memory fragmentation/pressure on T4 with PyTorch~2.8.
\end{itemize}


% =============================================================
\section{Roofline Modeling }
% =============================================================

\subsection{Hardware Roofline}
We characterize each environment with approximate peak compute (GFLOPs/s) and memory bandwidth (GB/s). These peaks define the roofline limits used in the following figures.
\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
Device & Peak FP32 (GFLOPs/s) & Mem BW (GB/s) \\
\midrule
Apple M3 Pro (MPS) & 5700 & 150 \\
Cloud CPU (4 vCPU, approx.) & 160 & 30 \\
NVIDIA Tesla T4 (CUDA) & 8100 & 320 \\
\bottomrule
\end{tabular}
\caption{Peak numbers used for roofline modeling (CPU values are approximate).}
\end{table}

\subsection{Workload Placement}
\subsubsection*{Device Rooflines}
% Place (a) and (b) together to save space
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{roofline_device_mps}
        \caption{MPS}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{roofline_device_cpu}
        \caption{CPU}
    \end{subfigure}
    \caption{Device rooflines with model points.}
\end{figure}

% Force next page, then continue with (c) on its own
\clearpage
\begin{figure}[H]\ContinuedFloat
    \centering
    \setcounter{subfigure}{2}% continue subfigure labels to (c)
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{roofline_device_cuda}
        \caption{T4 (CUDA)}
    \end{subfigure}
    \caption{Device rooflines with model points (continued).}
\end{figure}

\subsubsection*{Per-Model Rooflines}
\begin{figure}[H]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{roofline_model_resnet18}
        \caption{ResNet18}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{roofline_model_mobilenet_v2}
        \caption{MobileNetV2}
    \end{subfigure}

    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{roofline_model_resnet50}
        \caption{ResNet50}
    \end{subfigure}\hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{roofline_model_squeezenet1_1}
        \caption{SqueezeNet1.1}
    \end{subfigure}
    \caption{Per-model rooflines across devices.}
\end{figure}

\subsection{Batch Size Impact on Roofline}
Increasing batch size generally improves throughput and thus achieved GFLOPs/s (vertical movement toward the roof) and can slightly alter effective arithmetic intensity through better cache/memory behavior. On the local MPS device, SqueezeNet1.1 shows strong gains from batch 128 to 256, with regression at 512 likely due to memory pressure/fragmentation.

\subsection{Roofline Interpretation (Detailed)}
\subsubsection*{Device-Level Observations}
\textbf{Cloud CPU (e2-standard-4).} Extremely low compute roof (\(~160\) GFLOPs/s). All models lie far below the compute roof and align along the memory-bandwidth slope $y=\mathrm{BW}\cdot \mathrm{AI}$; effectively memory-bound even for ResNet50.\\
\textbf{Apple M3 Pro (MPS).} Higher memory bandwidth than CPU and a moderate compute roof (\(~5.7\) TFLOPs). Small models (MobileNetV2, SqueezeNet) remain memory-bound; ResNet50 is closer to compute-bound but still below the roof due to limited kernel fusion opportunities in MPS.\\
\textbf{NVIDIA Tesla T4 (CUDA).} Highest compute roof (\(~8.1\) TFLOPs). MobileNetV2 and SqueezeNet lie on the bandwidth slope (memory-bound). ResNet18 approaches a sizable fraction of the memory roof, while ResNet50 moves toward the compute ceiling (\(~1.3\) TFLOPs, \(~16\%\) of peak).

\subsubsection*{Model-Level Observations}
\textbf{MobileNetV2.} Lowest AI $\Rightarrow$ always memory-bound; GPU speedup limited by memory bandwidth.\\
\textbf{SqueezeNet.} Memory-bound with strong batch-size sensitivity; scales until memory fragmentation/pressure appears at large batches.\\
\textbf{ResNet18.} Moderate AI; transitional model. GPU provides large gains and moves it toward the compute-bound region.\\
\textbf{ResNet50.} Most compute-heavy of the four. Only model that climbs significantly toward the compute roof on T4.


% =============================================================
\section{Analysis }
% =============================================================

\subsection{Key Findings}
\begin{enumerate}
  \item GPUs outperform CPUs substantially: from $\sim$10$\times$ on light models up to $\sim$50$\times$ on compute-heavy models.
  \item Apple M3 Pro (MPS) performance lands between CPU and NVIDIA GPU, closer to the GPU for many workloads.
  \item Architecture matters: MobileNetV2 and SqueezeNet are more memory-bound (smaller speedups), while ResNet50, with higher arithmetic intensity, gains more on GPU.
  \item Batch size has a major impact: larger batches improve throughput and arithmetic intensity, but overly large batches (e.g., 128 for SqueezeNet on T4) can destabilize training.
  \item Cloud CPU (e2-standard-4 class) is not suitable for deep learning training at scale: very slow and often memory-bound.
\end{enumerate}

\subsection{Environment Impact}
The performance tables indicate clear benefits from GPU acceleration, with workload-specific gains shaped by arithmetic intensity and memory behavior.

\subsection{Cross-Device Speedups}
We define speedup as
\begin{equation}
\text{Speedup} = \frac{\text{Throughput on Device A}}{\text{Throughput on Device B}}\, .
\end{equation}

\begin{table}[H]
\centering
\begin{tabular}{lccc}
\toprule
Model & GPU/M3 Speedup & GPU/CPU Speedup & M3/CPU Speedup \\
\midrule
ResNet18 & 2.87$\times$ & 35.5$\times$ & 12.4$\times$ \\
MobileNetV2 & 2.51$\times$ & 21.4$\times$ & 8.5$\times$ \\
ResNet50 & 1.93$\times$ & 29.2$\times$ & 15.0$\times$ \\
SqueezeNet1.1 & 1.95$\times$ & 14.2$\times$ & 7.3$\times$ \\
\bottomrule
\end{tabular}
\caption{Cross-device speedups computed from Scenario~A throughputs.}
\end{table}

GPU provides 10--50$\times$ acceleration depending on model complexity; compute-heavy models (ResNet50) benefit most. The M3 Pro offers substantial acceleration over CPU (\(\sim 7\times\text{--}15\times\)), especially for deeper models. Lightweight architectures (MobileNetV2, SqueezeNet) saturate memory bandwidth and show smaller GPU gains.

\subsubsection*{Why T4 Performs Better Than M3 Pro}
\begin{itemize}
  \item Discrete GDDR6 memory (\(\sim 320\) GB/s) vs. unified memory on M3 Pro (\(\sim 150\) GB/s).
  \item CUDA kernel fusion, tensor-core support, and mature scheduling/optimizations.
  \item MPS backend continues to improve but lacks CUDA's breadth of operator-level tuning.
\end{itemize}

\subsubsection*{Why CPU Performs Poorly}
\begin{itemize}
  \item Limited memory bandwidth (\(\sim 30\) GB/s) and smaller caches.
  \item Python input pipeline overhead and single-threaded bottlenecks.
  \item Lack of highly-optimized, vectorized conv kernels comparable to cuDNN or MPSGraph.
\end{itemize}

\subsection{Model Differences}
MobileNetV2 and SqueezeNet (depthwise and fire modules) exhibit lower arithmetic intensity and tend to be memory-bound; ResNet50 benefits more from GPU due to higher compute per byte.

\subsection{Batch Size Effects}
Larger batches generally improve utilization and throughput; however, stability and memory fragmentation can limit maximum usable batch sizes on specific devices.

\subsection{Bottlenecks Identified}
Memory bandwidth and input pipeline time dominate for lighter models; compute is the limiting factor for deeper networks at moderate batch sizes.


% =============================================================
\section{Conclusion}
% =============================================================

This study benchmarked four neural network architectures across three compute environments local M3 Pro, cloud CPU, and NVIDIA T4 GPU using identical workloads and standardized logging. The results show clear trends consistent with roofline theory:
\begin{enumerate}
    \item GPUs offer the highest performance, achieving large speedups over cloud CPUs and consistently outperforming the M3 Pro.
    \item Model architecture strongly influences scaling: compute-heavy models (ResNet50) benefit most from GPU compute, while memory-bound models (MobileNetV2, SqueezeNet) saturate memory bandwidth and exhibit lower relative speedups.
    \item The M3 Pro performs as a mid-tier accelerator, offering strong throughput for small and medium-sized models but lacking the peak compute needed for large models.
    \item Batch size is a key determinant of utilization, with larger batches improving arithmetic intensity and throughput though overly large batches may cause instability or memory fragmentation.
    \item Roofline plots clearly reveal the bottlenecks: cloud CPU is always memory-bound, MPS is mixed-bound depending on model, and T4 is compute-bound for deep networks.
\end{enumerate}

Overall, the GPU environment provides the most scalable and efficient platform for neural network training. The roofline model serves as an effective tool for understanding these performance differences and guiding future optimization strategies, including mixed-precision training, multi-GPU scaling, and improved input pipelines.

\end{document}
